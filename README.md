# MSM (Multi-Scalar Multiplication) GPU Implementation

This repository contains optimized GPU implementations of Multi-Scalar Multiplication (MSM) for elliptic curve cryptography, featuring both single GPU and multi-GPU distributed approaches.

## ğŸ“ Project Structure

```
â”œâ”€â”€ src/                    # Source code files
â”‚   â”œâ”€â”€ main.cu            # Multi-GPU distributed implementation
â”‚   â”œâ”€â”€ main_single_gpu.cu # Single GPU implementation
â”‚   â”œâ”€â”€ kernel.cu          # CUDA kernel implementations
â”‚   â”œâ”€â”€ kernel.cuh         # CUDA kernel headers
â”‚   â”œâ”€â”€ common.cu          # Common utility functions
â”‚   â””â”€â”€ common.cuh         # Common headers and definitions
â”œâ”€â”€ executables/           # Compiled binaries
â”‚   â”œâ”€â”€ msm_multigpu       # Multi-GPU executable
â”‚   â”œâ”€â”€ msm_single_gpu     # Single GPU executable
â”‚   â””â”€â”€ msm_smallcheck     # Legacy executable
â”œâ”€â”€ scripts/               # Analysis and plotting scripts
â”‚   â”œâ”€â”€ plot_comparison.py # Single vs Multi-GPU comparison
â”‚   â”œâ”€â”€ plot_multi_gpu.py  # Multi-GPU performance analysis
â”‚   â”œâ”€â”€ plot_performance.py # General performance plotting
â”‚   â””â”€â”€ run_performance_test.sh # Performance testing script
â”œâ”€â”€ data/                  # Performance data files
â”‚   â”œâ”€â”€ multi_gpu_timing.csv
â”‚   â”œâ”€â”€ single_gpu_timing.csv
â”‚   â””â”€â”€ test_timing.csv
â”œâ”€â”€ plots/                 # Generated plots and visualizations
â”‚   â”œâ”€â”€ gpu_comparison.png
â”‚   â”œâ”€â”€ multi_gpu_performance.png
â”‚   â””â”€â”€ msm_performance_comparison.png
â”œâ”€â”€ build/                 # Build artifacts
â”œâ”€â”€ CMakeLists.txt         # CMake configuration
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md             # This file
```

## ğŸš€ Quick Start

### Prerequisites
- NVIDIA GPU(s) with CUDA support
- CUDA Toolkit (version 11.0 or higher)
- Python 3.7+ with required packages (see requirements.txt)

### Compilation

#### Multi-GPU Implementation
```bash
cd src
nvcc -O2 main.cu kernel.cu common.cu -o ../executables/msm_multigpu
```

#### Single GPU Implementation
```bash
cd src
nvcc -O2 main_single_gpu.cu kernel.cu common.cu -o ../executables/msm_single_gpu
```

### Running Tests

#### Toy Test (Correctness Check)
```bash
# Multi-GPU toy test
./executables/msm_multigpu --toy2

# Single GPU toy test
./executables/msm_single_gpu --toy2
```

#### Performance Benchmarking
```bash
# Multi-GPU performance test
./executables/msm_multigpu

# Single GPU performance test
./executables/msm_single_gpu

# Custom problem sizes
./executables/msm_multigpu --N=64,128,256,512,1024
```

## ğŸ“Š Performance Analysis

### Generate Performance Plots
```bash
# Compare single vs multi-GPU performance
python3 scripts/plot_comparison.py

# Analyze multi-GPU performance only
python3 scripts/plot_multi_gpu.py

# General performance analysis
python3 scripts/plot_performance.py
```

### Automated Performance Testing
```bash
# Run comprehensive performance tests
bash scripts/run_performance_test.sh
```

## ğŸ”§ Implementation Details

### Multi-GPU Distributed Approach
- **Bucket Distribution**: Evenly distributes buckets across available GPUs
- **Load Balancing**: Handles cases where total buckets don't divide evenly
- **Result Combination**: Combines partial results from each GPU on CPU
- **Memory Efficiency**: Each GPU processes only its assigned subset

### Single GPU Approach
- **Simplified Architecture**: All buckets processed on one GPU
- **Streamlined Memory Management**: Single device allocation
- **Direct Timing Measurements**: No aggregation needed

### Key Features
- **Elliptic Curve Arithmetic**: Point addition, doubling, scalar multiplication
- **Windowed MSM**: 8-bit window size for optimal performance
- **Bucket-based Algorithm**: Efficient grouping of similar operations
- **CUDA Optimization**: Parallel processing with custom kernels

## ğŸ“ˆ Performance Metrics

The implementations measure and report:
- **Total GPU Time**: Complete execution time
- **GPU Compute Time**: Pure computation (bucket + window sums)
- **Memory Transfer Time**: Host-to-device and device-to-host transfers
- **Bucket Sum Time**: Time for bucket-level point additions
- **Window Sum Time**: Time for window-level weighted sums

## ğŸ¯ Command Line Options

```
Usage: msm_multigpu [Options]
  -n, --N <sizes>     Comma-separated problem sizes (default: 64,128,256,...)
  --csv=<file>        Output CSV file name
  --print-input       Print input values for debugging
  --toy2              Run simple toy test (3*G + 5*G = 8*G)
  -h, --help          Show this help message
```

## ğŸ“‹ Output Files

### CSV Data Format
```
N,GPU_Total,GPU_Compute,GPU_Transfer,GPU_Bucket,GPU_Window
64,1234,1000,234,800,200
128,2345,2000,345,1600,400
...
```

### Generated Plots
- **gpu_comparison.png**: Single vs Multi-GPU performance comparison
- **multi_gpu_performance.png**: Detailed multi-GPU analysis
- **msm_performance_comparison.png**: General performance visualization

## ğŸ” Analysis Scripts

### plot_comparison.py
- **Main Plot**: Total execution time comparison
- **Speedup Analysis**: Shows multi-GPU speedup over single GPU
- **Console Analysis**: Detailed performance statistics

### plot_multi_gpu.py
- **4-Panel Analysis**: Total time, operation breakdown, transfer overhead, throughput
- **Comprehensive Metrics**: Detailed multi-GPU performance analysis

### plot_performance.py
- **General Analysis**: Works with any timing CSV file
- **Base-2 Log Scale**: Optimized for power-of-2 problem sizes

## ğŸ› ï¸ Development

### Adding New Features
1. Modify source files in `src/`
2. Update compilation commands
3. Test with toy examples first
4. Run performance benchmarks
5. Update documentation

### Debugging
- Use `--print-input` flag for detailed input inspection
- Run toy tests for correctness verification
- Check CSV output for timing breakdowns

## ğŸ“š Technical Background

### MSM Algorithm
Multi-Scalar Multiplication computes: `âˆ‘(i=0 to N-1) scalar[i] * base[i]`

### Windowed Approach
- **Window Size**: 8 bits (configurable)
- **Buckets**: 255 buckets per window (1-255)
- **Windows**: 8 windows for 64-bit scalars

### GPU Optimization
- **Parallel Bucket Processing**: Each thread handles one bucket
- **Memory Coalescing**: Optimized memory access patterns
- **Kernel Fusion**: Combined bucket and window operations

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- CUDA programming best practices
- Elliptic curve cryptography fundamentals
- Performance optimization techniques 